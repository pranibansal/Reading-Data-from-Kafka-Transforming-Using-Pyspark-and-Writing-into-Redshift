{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a05cc97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "696d8bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init('/Users/pranibansal/Downloads/spark-3.4.0-bin-hadoop3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "966bc438",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "031f1cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/pranibansal/Downloads/spark-3.4.0-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/pranibansal/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/pranibansal/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-d0f97d55-b3c0-41b3-b3ad-d6bc761d5994;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.32 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.2 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in local-m2-cache\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.2 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in local-m2-cache\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.3.0/spark-sql-kafka-0-10_2.12-3.3.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0!spark-sql-kafka-0-10_2.12.jar (126ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.3.0/spark-token-provider-kafka-0-10_2.12-3.3.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0!spark-token-provider-kafka-0-10_2.12.jar (96ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/2.8.1/kafka-clients-2.8.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kafka#kafka-clients;2.8.1!kafka-clients.jar (230ms)\n",
      "downloading file:/Users/pranibansal/.m2/repository/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar ...\n",
      "\t[SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.0!jsr305.jar (2ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-pool2;2.11.1!commons-pool2.jar (60ms)\n",
      "downloading file:/Users/pranibansal/.m2/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...\n",
      "\t[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (3ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.3.2/hadoop-client-runtime-3.3.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-client-runtime;3.3.2!hadoop-client-runtime.jar (1007ms)\n",
      "downloading https://repo1.maven.org/maven2/org/lz4/lz4-java/1.8.0/lz4-java-1.8.0.jar ...\n",
      "\t[SUCCESSFUL ] org.lz4#lz4-java;1.8.0!lz4-java.jar (73ms)\n",
      "downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.8.4/snappy-java-1.1.8.4.jar ...\n",
      "\t[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.8.4!snappy-java.jar(bundle) (127ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.32/slf4j-api-1.7.32.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.32!slf4j-api.jar (57ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.3.2/hadoop-client-api-3.3.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-client-api;3.3.2!hadoop-client-api.jar (628ms)\n",
      "downloading https://repo1.maven.org/maven2/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar ...\n",
      "\t[SUCCESSFUL ] commons-logging#commons-logging;1.1.3!commons-logging.jar (74ms)\n",
      ":: resolution report :: resolve 7335ms :: artifacts dl 2518ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from local-m2-cache in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.32 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from local-m2-cache in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   12  |   12  |   12  |   0   ||   12  |   12  |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-d0f97d55-b3c0-41b3-b3ad-d6bc761d5994\n",
      "\tconfs: [default]\n",
      "\t12 artifacts copied, 0 already retrieved (56631kB/82ms)\n",
      "23/06/04 16:16:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/06/04 16:16:44 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Streaming from Kafka\") \\\n",
    "    .config(\"spark.streaming.stopGracefullyOnShutdown\", True) \\\n",
    "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0') \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", 4) \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99f5c749",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import StructType,StructField \n",
    "from pyspark.sql.types import StringType, IntegerType, ArrayType\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bc7bc5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark \\\n",
    "    .read.format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"first_topic\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load() \\\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f7ea5927",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/04 16:57:32 WARN AdminClientConfig: The configuration 'key.deserializer' was supplied but isn't a known config.\n",
      "23/06/04 16:57:32 WARN AdminClientConfig: The configuration 'value.deserializer' was supplied but isn't a known config.\n",
      "23/06/04 16:57:32 WARN AdminClientConfig: The configuration 'enable.auto.commit' was supplied but isn't a known config.\n",
      "23/06/04 16:57:32 WARN AdminClientConfig: The configuration 'max.poll.records' was supplied but isn't a known config.\n",
      "23/06/04 16:57:32 WARN AdminClientConfig: The configuration 'auto.offset.reset' was supplied but isn't a known config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-----------+---------+------+--------------------+-------------+\n",
      "|    key|               value|      topic|partition|offset|           timestamp|timestampType|\n",
      "+-------+--------------------+-----------+---------+------+--------------------+-------------+\n",
      "|   [30]|[7B 22 69 64 22 3...|first_topic|        0|     0|2023-06-04 15:53:...|            0|\n",
      "|   [31]|[7B 22 69 64 22 3...|first_topic|        0|     1|2023-06-04 15:53:...|            0|\n",
      "|   [32]|[7B 22 69 64 22 3...|first_topic|        0|     2|2023-06-04 15:53:...|            0|\n",
      "|   [33]|[7B 22 69 64 22 3...|first_topic|        0|     3|2023-06-04 15:53:...|            0|\n",
      "|   [34]|[7B 22 69 64 22 3...|first_topic|        0|     4|2023-06-04 15:53:...|            0|\n",
      "|   [35]|[7B 22 69 64 22 3...|first_topic|        0|     5|2023-06-04 15:53:...|            0|\n",
      "|   [36]|[7B 22 69 64 22 3...|first_topic|        0|     6|2023-06-04 15:53:...|            0|\n",
      "|   [37]|[7B 22 69 64 22 3...|first_topic|        0|     7|2023-06-04 15:53:...|            0|\n",
      "|   [38]|[7B 22 69 64 22 3...|first_topic|        0|     8|2023-06-04 15:53:...|            0|\n",
      "|   [39]|[7B 22 69 64 22 3...|first_topic|        0|     9|2023-06-04 15:53:...|            0|\n",
      "|[31 30]|[7B 22 69 64 22 3...|first_topic|        0|    10|2023-06-04 15:53:...|            0|\n",
      "|[31 31]|[7B 22 69 64 22 3...|first_topic|        0|    11|2023-06-04 15:53:...|            0|\n",
      "|[31 32]|[7B 22 69 64 22 3...|first_topic|        0|    12|2023-06-04 15:53:...|            0|\n",
      "|[31 33]|[7B 22 69 64 22 3...|first_topic|        0|    13|2023-06-04 15:53:...|            0|\n",
      "|[31 34]|[7B 22 69 64 22 3...|first_topic|        0|    14|2023-06-04 15:53:...|            0|\n",
      "|[31 35]|[7B 22 69 64 22 3...|first_topic|        0|    15|2023-06-04 15:53:...|            0|\n",
      "|[31 36]|[7B 22 69 64 22 3...|first_topic|        0|    16|2023-06-04 15:53:...|            0|\n",
      "|[31 37]|[7B 22 69 64 22 3...|first_topic|        0|    17|2023-06-04 15:53:...|            0|\n",
      "|[31 38]|[7B 22 69 64 22 3...|first_topic|        0|    18|2023-06-04 15:53:...|            0|\n",
      "|[31 39]|[7B 22 69 64 22 3...|first_topic|        0|    19|2023-06-04 15:53:...|            0|\n",
      "+-------+--------------------+-----------+---------+------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d96d53f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.withColumn(\"json\", from_json(df[\"json\"], jsonSchema)) \\\n",
    "#                 .select(\"json.*\")\n",
    "\n",
    "# Parse value from binay to string\n",
    "json_df = df.selectExpr(\"cast(value as string) as value\")\n",
    "\n",
    "# # Apply Schema to JSON value column and expand the value\n",
    "# from pyspark.sql.functions import from_json\n",
    "\n",
    "# json_expanded_df = json_df.withColumn(\"value\", from_json(json_df[\"value\"], json_schema)).select(\"value.*\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4bcd5488",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonSchema = StructType([StructField(\"id\", StringType(), True), StructField(\"Year\", StringType(), True),\n",
    "                                     StructField(\"Reporting_Airline\", StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e5a25fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "json_expanded_df = json_df.withColumn(\"value\", from_json(json_df[\"value\"], jsonSchema)).select(\"value.*\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8745f77b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/04 16:58:17 WARN AdminClientConfig: The configuration 'key.deserializer' was supplied but isn't a known config.\n",
      "23/06/04 16:58:17 WARN AdminClientConfig: The configuration 'value.deserializer' was supplied but isn't a known config.\n",
      "23/06/04 16:58:17 WARN AdminClientConfig: The configuration 'enable.auto.commit' was supplied but isn't a known config.\n",
      "23/06/04 16:58:17 WARN AdminClientConfig: The configuration 'max.poll.records' was supplied but isn't a known config.\n",
      "23/06/04 16:58:17 WARN AdminClientConfig: The configuration 'auto.offset.reset' was supplied but isn't a known config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----------------+\n",
      "| id|Year|Reporting_Airline|\n",
      "+---+----+-----------------+\n",
      "|  0|1998|               NW|\n",
      "|  1|2009|               FL|\n",
      "|  2|2013|               MQ|\n",
      "|  3|2010|               DL|\n",
      "|  4|2006|               US|\n",
      "|  5|1995|               DL|\n",
      "|  6|2006|               CO|\n",
      "|  7|2019|               9E|\n",
      "|  8|2008|               YV|\n",
      "|  9|2018|               WN|\n",
      "| 10|1991|               US|\n",
      "| 11|2014|               WN|\n",
      "| 12|1994|               AA|\n",
      "| 13|2013|               OO|\n",
      "| 14|2003|               UA|\n",
      "| 15|1988|               PI|\n",
      "| 16|2007|               NW|\n",
      "| 17|2015|               AS|\n",
      "| 18|2006|               UA|\n",
      "| 19|2017|               WN|\n",
      "+---+----+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_expanded_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2af5acad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/04 16:58:42 WARN AdminClientConfig: The configuration 'key.deserializer' was supplied but isn't a known config.\n",
      "23/06/04 16:58:42 WARN AdminClientConfig: The configuration 'value.deserializer' was supplied but isn't a known config.\n",
      "23/06/04 16:58:42 WARN AdminClientConfig: The configuration 'enable.auto.commit' was supplied but isn't a known config.\n",
      "23/06/04 16:58:42 WARN AdminClientConfig: The configuration 'max.poll.records' was supplied but isn't a known config.\n",
      "23/06/04 16:58:42 WARN AdminClientConfig: The configuration 'auto.offset.reset' was supplied but isn't a known config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|Reporting_Airline|count|\n",
      "+-----------------+-----+\n",
      "|               US|   40|\n",
      "|               YV|    6|\n",
      "|               WN|   72|\n",
      "|               PI|    4|\n",
      "|               EV|   11|\n",
      "|               HP|   11|\n",
      "|               NW|   25|\n",
      "|               MQ|   22|\n",
      "|               DL|   77|\n",
      "|               OO|   27|\n",
      "|               UA|   39|\n",
      "|               XE|    8|\n",
      "|               EA|    1|\n",
      "|               YX|    2|\n",
      "|               TW|    9|\n",
      "|               B6|    7|\n",
      "|               F9|    4|\n",
      "|               PS|    1|\n",
      "|               FL|    5|\n",
      "|               CO|   31|\n",
      "+-----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_expanded_df.select('Reporting_Airline').groupBy('Reporting_Airline').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5db3cec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = json_expanded_df.select('id','Year','Reporting_Airline').filter('Year >= 2015')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
